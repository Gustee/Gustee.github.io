<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 分类问题的模型衡量指标 · 菜鸡Gustee的挣扎</title><meta name="description" content="分类问题的模型衡量指标 - Gustee"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://yoursite.com/atom.xml" title="菜鸡Gustee的挣扎"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/gustee" target="_blank" class="nav-list-link">GITHUB</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">分类问题的模型衡量指标</h1><div class="post-info">Jun 2, 2019</div><div class="post-content"><p>分类问题的性能衡量指标</p>
<p>本文主要讨论二分类问题中的模型性能衡量指标。</p>
<h2 id="准确率"><a href="#准确率" class="headerlink" title="准确率"></a>准确率</h2><p>首先就是我们最熟悉的准确率。<br><strong>准确率（accuracy）：</strong> 预测正确的结果占总样本的百分比。<br>虽然准确率可以判断总的正确率，但是在样本不平衡的情况下，也有明显的缺点。以下面的例子来说明准确率的存在的问题：<br>假设我们的模型是一个癌症预测系统，输入体检信息，判断是否有癌症。假设该癌症的患病率为0.1%，如果我们的模型什么都不做直接预测的所有人都是健康的，那么我们的模型可以达到99.9%的准确率，然而我们的模型并不是一个性能很好的模型。<code>所以对于极度偏斜的数据，尤其是我们关注minority class时，准确率基本没有参考价值，并不能很好的衡量模型的好坏。</code></p>
<h2 id="精准率和召回率"><a href="#精准率和召回率" class="headerlink" title="精准率和召回率"></a>精准率和召回率</h2><h3 id="混淆矩阵-Confusion-Matrix"><a href="#混淆矩阵-Confusion-Matrix" class="headerlink" title="混淆矩阵 Confusion Matrix"></a>混淆矩阵 Confusion Matrix</h3><p>为了更好的定义精准率和召回率，我们先介绍<strong>混淆矩阵(confusion matrix)</strong><br>我们把如下的矩阵称为混淆矩阵<br><img src="/分类问题的模型衡量指标/confusion.png" alt="confusion"><br>矩阵的每一行代表预测值，每一行代表样本的真是标签。<br>我们用一个例子来理解混淆矩阵：<br>有一个癌症预测的任务，测试集为10000人，其中有9990个负样本，即没有患有癌症，为健康的用户，有10个正样本，患有癌症。经过模型的预测，在9990个健康用户中，其中有9978个健康用户被预测正确，剩下的12个用户被错误预测为患有癌症；在10个癌症患者中，其中有8个用户被预测正确，剩下的2个患者被错误预测为健康。根据上述数据可以得到其的混淆矩阵：<br><img src="/分类问题的模型衡量指标/1.png" alt="1"><br><strong>注意：我们一般把我们关注的类别记为正类，即Positive类</strong></p>
<h3 id="精准率与召回率"><a href="#精准率与召回率" class="headerlink" title="精准率与召回率"></a>精准率与召回率</h3><p><strong>精确率（Precision）：</strong> 又叫查准率计算方式如下：</p>
<script type="math/tex; mode=display">
precision=\frac{TP}{TP+FP}</script><p>根据上述计算方式，上例中的$precision=\frac{8}{8+12}=40\%$<br><code>精准率是针对预测结果而言的，它的含义是在所有被预测为正的样本中实际为正样本的概率，意思就是在预测为正样本的结果中，我们有多少把握可以预测正确。</code><br><strong>召回率（recall）：</strong> 又叫查全率，计算方式如下：</p>
<script type="math/tex; mode=display">
recall=\frac{TP}{TP+FN}</script><p>根据上述计算方式，上例中的$recall=\frac{8}{8+2}=80\%$<br>事实上，<code>召回率是针对原样本而言的，它的含义是在实际为正的样本中被预测为正样本的概率，假设测试集里面有100个正例，你的模型能预测覆盖到多少，如果你的模型预测到了40个正例，那你的recall就是40%。</code><br>下面这个图可以更好的理解精准率和召回率，<br><img src="/分类问题的模型衡量指标/precision%20recall.png" alt="precision recall"></p>
<h2 id="Precision-Recall的平衡"><a href="#Precision-Recall的平衡" class="headerlink" title="Precision-Recall的平衡"></a>Precision-Recall的平衡</h2><p>然而一般来说，precision和recall不可兼得，选择衡量指标还是要结合具体的使用场景。比如下面两个场景：<br>1、. 地震的预测<br>对于地震的预测，我们希望的是recall非常高，也就是说每次地震我们都希望预测出来。这个时候我们可以牺牲precision。情愿发出1000次警报，把10次地震都预测正确了；也不要预测100次对了8次漏了两次。<br>2. 嫌疑人定罪<br>基于不错怪一个好人的原则，对于嫌疑人的定罪我们希望是非常准确的。及时有时候放过了一些罪犯（recall低），但也是值得的。<br>以具体的模型为例：<br>我们知道在逻辑回归样本的预测概率大于0.5时，被判定为正类，否则被判定为父类，我们可以通过调整该概率阀值就可以平衡Precision和Recall，如下图所示。<br><img src="/分类问题的模型衡量指标/2.png" alt="2"><br>所以在上面的场景1中，我们为了更高的Recall，可以让概率阀值略小于0.5；在场景2中，我们为了更高的Precision，可以让概率阀值略大于0.5。</p>
<h3 id="P-R曲线"><a href="#P-R曲线" class="headerlink" title="P-R曲线"></a>P-R曲线</h3><p>根据模型不同的阀值，可以得到不同的Precision和对应Recall，然后以这些Precision为横坐标，Recall为纵坐标绘制出的曲线，称为Precision-Recall曲线或P-R曲线<br><img src="/分类问题的模型衡量指标/3.png" alt="3"></p>
<h2 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1 Score"></a>F1 Score</h2><p>上述说道precision和recall不可兼得，那么F1 score就是一个综合考虑precision和recall的指标，定义如下：<br><strong>F1 Score</strong>是精准率和召回率的调和平均值</p>
<script type="math/tex; mode=display">
\frac{1}{F1} = \frac{1}{2}(\frac{1}{precision}+\frac{1}{recall})
\\F1 = \frac{2·precision·recall}{precision+recall}</script><h2 id="ROC-曲线与AUC"><a href="#ROC-曲线与AUC" class="headerlink" title="ROC 曲线与AUC"></a>ROC 曲线与AUC</h2><h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p><strong>ROC曲线（Receiver Operation Characteristic Curve)</strong> 描述的是TPR（True positive rate）和FPR（False positive rate）之前的关系。</p>
<script type="math/tex; mode=display">
TPR=\frac{TP}{TP+FN}
\\FPR=\frac{FP}{TN+FP}</script><p><strong>注意：从TPR的计算方式可以看出，TPR就是我们前文讨论的Recall</strong><br>与P-R曲线相同，取不同的threshold可以得到不同的TPR和FPR，然后以 FPR为横坐标，TPR为纵坐标画出的曲线即为ROC曲线。<br><img src="/分类问题的模型衡量指标/4.png" alt="4"></p>
<h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><p><strong>AUC（Area Under Curve）</strong> 为ROC曲线下面的面积。<br>理想模型十分准确，TPR与FPR均等于1，则AUC为1。然而在真实模型中，AUC均在0.5到1之间，AUC越高，则模型分类效果越好。</p>
<p>参考：<br><a href="https://tracholar.github.io/machine-learning/2018/01/26/auc.html" target="_blank" rel="noopener">https://tracholar.github.io/machine-learning/2018/01/26/auc.html</a><br><a href="http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/11/20/understanding-ROC-and-AUC" target="_blank" rel="noopener">http://vividfree.github.io/机器学习/2015/11/20/understanding-ROC-and-AUC</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/AUC若干计算方法及Python实现/" class="prev">PREV</a></div><div class="copyright"><p>© 2015 - 2019 <a href="http://yoursite.com">Gustee</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>